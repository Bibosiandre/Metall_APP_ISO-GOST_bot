import os
import logging
import hashlib
import json
import requests
import re
import asyncio
import aiohttp
import traceback
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, TimeoutError as FutureTimeoutError

import ollama
from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import (
    Application, CommandHandler, MessageHandler,
    filters, CallbackContext, CallbackQueryHandler,
    ContextTypes
)
import PyPDF2
import pdfplumber

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO,
    handlers=[
        logging.FileHandler('pdf_assistant.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
PDF_FOLDER = Path("pdf_documents")
PDF_FOLDER.mkdir(exist_ok=True)
OLLAMA_MODEL = "qwen2.5:14b-instruct-q4_K_M"
CACHE_FILE = "documents_cache.json"
OLLAMA_TIMEOUT = 60  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç –¥–ª—è Ollama
TELEGRAM_TIMEOUT = 30  # –¢–∞–π–º–∞—É—Ç –¥–ª—è Telegram
INTERNET_TIMEOUT = 10  # –¢–∞–π–º–∞—É—Ç –¥–ª—è –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–∑–∞–ø—Ä–æ—Å–æ–≤

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –ø—É–ª –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è —Ç—è–∂–µ–ª—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
executor = ThreadPoolExecutor(max_workers=4)


class AdvancedPDFProcessor:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ PDF —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –ø–æ–∏—Å–∫–æ–º"""

    def __init__(self):
        self.documents_cache: Dict[str, Dict] = {}
        self.chunk_index: Dict[str, List[Dict]] = {}
        self.load_cache()
        self.update_documents()

    def load_cache(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ–º –∫—ç—à –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        if os.path.exists(CACHE_FILE):
            try:
                with open(CACHE_FILE, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                    self.documents_cache = cache_data.get('documents', {})
                    self.chunk_index = cache_data.get('chunks', {})
                logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω –∫—ç—à: {len(self.documents_cache)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫—ç—à–∞: {e}")
                self.documents_cache = {}
                self.chunk_index = {}

    def save_cache(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ–º –∫—ç—à"""
        try:
            cache_data = {
                'documents': self.documents_cache,
                'chunks': self.chunk_index,
                'updated_at': datetime.now().isoformat()
            }
            with open(CACHE_FILE, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            logger.info(f"–ö—ç—à —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {CACHE_FILE}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫—ç—à–∞: {e}")

    def calculate_file_hash(self, file_path: Path) -> str:
        """–•–µ—à —Ñ–∞–π–ª–∞ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π"""
        hasher = hashlib.md5()
        with open(file_path, 'rb') as f:
            buf = f.read()
            hasher.update(buf)
        return hasher.hexdigest()

    def extract_text_advanced(self, file_path: Path) -> Tuple[str, Dict]:
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
        text = ""
        metadata = {
            "pages": 0,
            "sections": [],
            "tables_found": 0,
            "images_found": 0,
            "extraction_method": "unknown"
        }

        try:
            # –ú–µ—Ç–æ–¥ 1: pdfplumber (–ª—É—á—à–∏–π –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö PDF)
            with pdfplumber.open(file_path) as pdf:
                metadata["pages"] = len(pdf.pages)
                metadata["extraction_method"] = "pdfplumber"

                for i, page in enumerate(pdf.pages, 1):
                    try:
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –±–µ–∑ problematical parameters
                        page_text = page.extract_text()
                        if page_text:
                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–æ–∫—É–º–µ–Ω—Ç–∞
                            text += f"\n{'=' * 60}\n–°—Ç—Ä–∞–Ω–∏—Ü–∞ {i}\n{'=' * 60}\n{page_text}\n"

                            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ (—Å—Ç—Ä–æ–∫–∏ –≤ –≤–µ—Ä—Ö–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ)
                            lines = page_text.split('\n')
                            for line in lines:
                                clean_line = line.strip()
                                if (len(clean_line) > 3 and len(clean_line) < 100 and
                                        clean_line.isupper() and clean_line not in metadata["sections"]):
                                    metadata["sections"].append(clean_line)

                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ç–∞–±–ª–∏—Ü
                        try:
                            tables = page.extract_tables()
                            if tables:
                                metadata["tables_found"] += len(tables)
                                text += f"\n[–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ —Ç–∞–±–ª–∏—Ü –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {i}: {len(tables)}]\n"
                        except Exception as e:
                            logger.debug(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–∞–±–ª–∏—Ü: {e}")

                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
                        if page.images:
                            metadata["images_found"] += len(page.images)
                            text += f"\n[–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {i}: {len(page.images)}]\n"

                    except Exception as e:
                        logger.warning(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {i}: {e}")
                        continue

                return text, metadata

        except Exception as e:
            logger.warning(f"pdfplumber error: {e}")
            try:
                # –ú–µ—Ç–æ–¥ 2: PyPDF2 (—Ä–µ–∑–µ—Ä–≤–Ω—ã–π)
                with open(file_path, 'rb') as file:
                    reader = PyPDF2.PdfReader(file)
                    metadata["pages"] = len(reader.pages)
                    metadata["extraction_method"] = "pypdf2"

                    for i, page in enumerate(reader.pages, 1):
                        page_text = page.extract_text()
                        if page_text:
                            text += f"\n{'=' * 60}\n–°—Ç—Ä–∞–Ω–∏—Ü–∞ {i}\n{'=' * 60}\n{page_text}\n"

                    return text, metadata

            except Exception as e2:
                logger.error(f"PyPDF2 error: {e2}")
                return "", metadata

    def chunk_text_intelligently(self, text: str, filename: str) -> List[Dict]:
        """–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏ —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        if not text:
            return []

        chunks = []

        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º (–µ—Å–ª–∏ –µ—Å—Ç—å –º–∞—Ä–∫–µ—Ä—ã —Å—Ç—Ä–∞–Ω–∏—Ü)
        page_markers = re.split(r'\n={10,}\n–°—Ç—Ä–∞–Ω–∏—Ü–∞ \d+\n={10,}\n', text)

        if len(page_markers) > 1:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º
            for i, page_text in enumerate(page_markers[1:], 1):
                if page_text.strip():
                    # –†–∞–∑–±–∏–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –Ω–∞ –∞–±–∑–∞—Ü—ã
                    paragraphs = re.split(r'\n\s*\n', page_text)
                    current_chunk = ""

                    for para in paragraphs:
                        if len(current_chunk) + len(para) < 1500:
                            current_chunk += para + "\n\n"
                        else:
                            if current_chunk.strip():
                                chunks.append({
                                    "text": current_chunk.strip(),
                                    "page": i,
                                    "source": filename,
                                    "chunk_type": "page_section"
                                })
                            current_chunk = para + "\n\n"

                    if current_chunk.strip():
                        chunks.append({
                            "text": current_chunk.strip(),
                            "page": i,
                            "source": filename,
                            "chunk_type": "page_section"
                        })
        else:
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–º—ã—Å–ª–æ–≤—ã–µ –±–ª–æ–∫–∏
            sentences = re.split(r'(?<=[.!?])\s+', text)
            current_chunk = ""

            for sentence in sentences:
                if len(current_chunk) + len(sentence) < 1000:
                    current_chunk += sentence + " "
                else:
                    if current_chunk.strip():
                        chunks.append({
                            "text": current_chunk.strip(),
                            "page": 0,
                            "source": filename,
                            "chunk_type": "semantic"
                        })
                    current_chunk = sentence + " "

            if current_chunk.strip():
                chunks.append({
                    "text": current_chunk.strip(),
                    "page": 0,
                    "source": filename,
                    "chunk_type": "semantic"
                })

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞
        for chunk in chunks:
            chunk["keywords"] = self.extract_keywords(chunk["text"])

        return chunks

    def extract_keywords(self, text: str, max_keywords: int = 10) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        # –£–±–∏—Ä–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Å–ª–æ–≤–∞
        stop_words = {
            '–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–æ—Ç', '–¥–æ', '–∏–∑', '–Ω–µ',
            '—á—Ç–æ', '—ç—Ç–æ', '–∫–∞–∫', '—Ç–∞–∫', '–∏–ª–∏', '–Ω–æ', '–∑–∞', '–∂–µ', '–±—ã',
            'the', 'and', 'of', 'to', 'in', 'a', 'is', 'that', 'for',
            'iso', '–≥–æ—Å—Ç', '—Å—Ç–∞–Ω–¥–∞—Ä—Ç', '–¥–æ–∫—É–º–µ–Ω—Ç', '—Å—Ç—Ä–∞–Ω–∏—Ü–∞'
        }

        # –ù–∞—Ö–æ–¥–∏–º —Å–ª–æ–≤–∞ (—Ä—É—Å—Å–∫–∏–µ –∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ)
        words = re.findall(r'\b[a-zA-Z–∞-—è–ê-–Ø—ë–Å]{3,}\b', text.lower())

        # –°—á–∏—Ç–∞–µ–º —á–∞—Å—Ç–æ—Ç—É
        from collections import Counter
        word_counts = Counter(words)

        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Å–ª–æ–≤–∞ –∏ –≤—ã–±–∏—Ä–∞–µ–º –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã–µ
        keywords = []
        for word, count in word_counts.most_common(20):
            if word not in stop_words and len(word) > 2:
                keywords.append(f"{word}:{count}")
                if len(keywords) >= max_keywords:
                    break

        return keywords

    def update_documents(self):
        """–û–±–Ω–æ–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""
        if not PDF_FOLDER.exists():
            PDF_FOLDER.mkdir()
            print(f"üìÅ –°–æ–∑–¥–∞–Ω–∞ –ø–∞–ø–∫–∞: {PDF_FOLDER}")
            return

        pdf_files = list(PDF_FOLDER.glob("*.pdf"))
        print(f"üìÅ –ù–∞–π–¥–µ–Ω–æ PDF —Ñ–∞–π–ª–æ–≤: {len(pdf_files)}")

        updated_count = 0
        self.chunk_index.clear()

        for pdf_file in pdf_files:
            try:
                file_hash = self.calculate_file_hash(pdf_file)
                filename = pdf_file.name

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –æ–±–Ω–æ–≤–ª—è—Ç—å
                if filename in self.documents_cache:
                    cached_hash = self.documents_cache[filename].get("file_hash", "")
                    if cached_hash == file_hash:
                        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —á–∞–Ω–∫–∏ –∏–∑ –∫—ç—à–∞
                        if filename in self.chunk_index:
                            continue

                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–æ–≤—ã–π/–∏–∑–º–µ–Ω–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                print(f"üìÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é: {filename}")
                text, metadata = self.extract_text_advanced(pdf_file)

                if text and len(text.strip()) > 100:
                    # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏
                    chunks = self.chunk_text_intelligently(text, filename)

                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
                    self.documents_cache[filename] = {
                        "file_hash": file_hash,
                        "metadata": metadata,
                        "text_preview": text[:1000],
                        "chunk_count": len(chunks),
                        "processed_at": datetime.now().isoformat(),
                        "file_size": pdf_file.stat().st_size
                    }

                    # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º —á–∞–Ω–∫–∏
                    self.chunk_index[filename] = chunks

                    updated_count += 1
                    print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω: {filename} ({metadata['pages']} —Å—Ç—Ä., {len(chunks)} —á–∞–Ω–∫–æ–≤)")
                else:
                    print(f"‚ö†Ô∏è –ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç –≤ —Ñ–∞–π–ª–µ: {filename}")

            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {pdf_file}: {e}")
                logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {pdf_file}: {e}")

        if updated_count > 0:
            self.save_cache()
            print(f"üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {updated_count}")

        print(f"üìö –í—Å–µ–≥–æ –≤ –∫—ç—à–µ: {len(self.documents_cache)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        total_chunks = sum(len(chunks) for chunks in self.chunk_index.values())
        print(f"üß© –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {total_chunks}")

    def search_with_semantic(self, question: str, max_results: int = 5) -> List[Dict]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ —á–∞–Ω–∫–∞–º"""
        question_lower = question.lower()
        question_words = set(re.findall(r'\b[a-zA-Z–∞-—è–ê-–Ø—ë–Å]{3,}\b', question_lower))

        results = []

        for filename, chunks in self.chunk_index.items():
            for chunk in chunks:
                chunk_text = chunk["text"].lower()
                chunk_keywords = chunk.get("keywords", [])

                # –í—ã—á–∏—Å–ª—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
                score = 0

                # 1. –ü–æ–∏—Å–∫ —Ç–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π —Å–ª–æ–≤
                for word in question_words:
                    if word in chunk_text:
                        score += 2

                # 2. –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º —á–∞–Ω–∫–∞
                for kw_entry in chunk_keywords:
                    kw = kw_entry.split(':')[0]
                    if kw in question_words:
                        score += 3

                # 3. –ü–æ–∏—Å–∫ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é –¥–æ–∫—É–º–µ–Ω—Ç–∞
                if any(word in filename.lower() for word in question_words):
                    score += 5

                # 4. –ü–æ–∏—Å–∫ –ø–æ –Ω–æ–º–µ—Ä—É –ì–û–°–¢/ISO
                doc_standard = self.extract_standard_number(filename)
                if doc_standard and doc_standard in question:
                    score += 10

                if score > 0:
                    results.append({
                        "score": score,
                        "text": chunk["text"],
                        "source": filename,
                        "page": chunk.get("page", 0),
                        "chunk_type": chunk.get("chunk_type", "unknown")
                    })

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        results.sort(key=lambda x: x["score"], reverse=True)

        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã (–ø–æ—Ö–æ–∂–∏–π —Ç–µ–∫—Å—Ç)
        unique_results = []
        seen_texts = set()

        for result in results[:max_results * 2]:  # –ë–µ—Ä–µ–º –±–æ–ª—å—à–µ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            text_hash = hashlib.md5(result["text"][:200].encode()).hexdigest()
            if text_hash not in seen_texts:
                seen_texts.add(text_hash)
                unique_results.append(result)
                if len(unique_results) >= max_results:
                    break

        return unique_results

    def extract_standard_number(self, filename: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–º–µ—Ä —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞"""
        patterns = [
            r'(–ì–û–°–¢\s*[0-9.-]+)',
            r'(ISO\s*[0-9.-]+)',
            r'(–°–¢\s*[0-9.-]+)',
            r'(EN\s*[0-9.-]+)',
            r'([0-9.-]+\s*–ì–û–°–¢)',
            r'([0-9.-]+\s*ISO)'
        ]

        for pattern in patterns:
            match = re.search(pattern, filename, re.IGNORECASE)
            if match:
                return match.group(1)
        return None

    async def search_internet_fallback(self, question: str) -> Optional[str]:
        """–ü–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ –∫–∞–∫ –∑–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç DuckDuckGo)"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º DuckDuckGo Instant Answer API
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=INTERNET_TIMEOUT)) as session:
                url = f"https://api.duckduckgo.com/"
                params = {
                    'q': question,
                    'format': 'json',
                    'no_html': '1',
                    'skip_disambig': '1'
                }

                async with session.get(url, params=params) as response:
                    if response.status == 200:
                        data = await response.json()

                        if data.get('AbstractText'):
                            return data['AbstractText']
                        elif data.get('RelatedTopics'):
                            first_topic = data['RelatedTopics'][0]
                            if isinstance(first_topic, dict) and 'Text' in first_topic:
                                return first_topic['Text'][:500]
                            elif isinstance(first_topic, str):
                                return first_topic[:500]

            return None

        except asyncio.TimeoutError:
            logger.warning(f"–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ: {question}")
            return None
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ: {e}")
            return None


class SmartPDFAssistant:
    """–£–º–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π PDF –∏ –¥–æ—Å—Ç—É–ø–æ–º –∫ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç—É"""

    def __init__(self, token: str):
        self.token = token
        self.processor = AdvancedPDFProcessor()
        self.application = None

    def check_ollama(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Ollama"""
        try:
            response = requests.get("http://localhost:11434/api/tags", timeout=10)

            if response.status_code == 200:
                data = response.json()
                models = data.get('models', [])

                model_names = []
                for model in models:
                    if 'name' in model:
                        model_names.append(model['name'])
                    elif 'model' in model:
                        model_names.append(model['model'])

                print(f"ü§ñ –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏: {', '.join(model_names)}")

                for name in model_names:
                    if OLLAMA_MODEL in name:
                        print(f"‚úÖ –ú–æ–¥–µ–ª—å {OLLAMA_MODEL} –Ω–∞–π–¥–µ–Ω–∞")
                        return True

                print(f"‚ùå –ú–æ–¥–µ–ª—å {OLLAMA_MODEL} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                return False
            else:
                print(f"‚ùå –û—à–∏–±–∫–∞ HTTP: {response.status_code}")
                return False

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama: {e}")
            return False

    async def ask_ollama_with_timeout(self, messages: List[Dict], timeout: int = OLLAMA_TIMEOUT) -> Dict:
        """–ó–∞–ø—Ä–æ—Å –∫ Ollama —Å —Ç–∞–π–º–∞—É—Ç–æ–º"""
        try:
            loop = asyncio.get_event_loop()
            response = await asyncio.wait_for(
                loop.run_in_executor(
                    executor,
                    lambda: ollama.chat(
                        model=OLLAMA_MODEL,
                        messages=messages,
                        options={
                            'temperature': 0.3,
                            'num_predict': 1200,
                            'num_thread': 4  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
                        }
                    )
                ),
                timeout=timeout
            )
            return response
        except asyncio.TimeoutError:
            logger.error(f"–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ Ollama (>{timeout} —Å–µ–∫)")
            raise
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ Ollama: {e}")
            raise

    async def ask_question_with_fallback(self, question: str) -> Tuple[str, str, bool]:
        """–ó–∞–¥–∞–µ–º –≤–æ–ø—Ä–æ—Å —Å fallback –Ω–∞ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç"""
        # –ü–æ–∏—Å–∫ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
        search_results = self.processor.search_with_semantic(question)

        if search_results:
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            context = "–ò–ù–§–û–†–ú–ê–¶–ò–Ø –ò–ó –î–û–ö–£–ú–ï–ù–¢–û–í:\n\n"
            sources = set()

            for i, result in enumerate(search_results, 1):
                context += f"–ò—Å—Ç–æ—á–Ω–∏–∫ {i}: {result['source']} (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {result['score']})\n"
                context += f"–¢–µ–∫—Å—Ç:\n{result['text'][:800]}...\n\n"
                sources.add(result['source'])

            sources_text = ", ".join(sources)

            prompt = f"""–¢—ã - —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —ç–∫—Å–ø–µ—Ä—Ç. –û—Ç–≤–µ—á–∞–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

{context}

–í–û–ü–†–û–°: {question}

–ò–ù–°–¢–†–£–ö–¶–ò–ò:
1. –û—Ç–≤–µ—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
2. –ë—É–¥—å —Ç–æ—á–Ω—ã–º –∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º
3. –£–ø–æ–º–∏–Ω–∞–π –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
4. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, –¥–æ–±–∞–≤—å —Å–≤–æ–∏ –∑–Ω–∞–Ω–∏—è

–û–¢–í–ï–¢:"""

            try:
                response = await self.ask_ollama_with_timeout(
                    messages=[
                        {"role": "system",
                         "content": "–¢—ã - —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —ç–∫—Å–ø–µ—Ä—Ç, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–æ—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é."},
                        {"role": "user", "content": prompt}
                    ]
                )
                answer = response['message']['content']
                answer += f"\n\nüìö *–ò—Å—Ç–æ—á–Ω–∏–∫–∏:* {sources_text}"
                return answer, "documents", True

            except asyncio.TimeoutError:
                logger.warning(f"–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–æ–ø—Ä–æ—Å–∞ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏: {question}")
                # –ü—Ä–æ–±—É–µ–º –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                try:
                    simple_response = await self.ask_ollama_with_timeout(
                        messages=[
                            {"role": "system", "content": "–¢—ã - —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —ç–∫—Å–ø–µ—Ä—Ç."},
                            {"role": "user", "content": question}
                        ],
                        timeout=15
                    )
                    answer = simple_response['message']['content']
                    answer += f"\n\nüìö *–ò—Å—Ç–æ—á–Ω–∏–∫–∏:* {sources_text}\n‚ö†Ô∏è *–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:* –û—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –±–µ–∑ –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑-–∑–∞ —Ç–∞–π–º–∞—É—Ç–∞"
                    return answer, "documents_timeout", True
                except:
                    # –ï—Å–ª–∏ –∏ —ç—Ç–æ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–æ, –∏—â–µ–º –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ
                    pass

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ Ollama: {e}")

        # –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∏–ª–∏ –æ—à–∏–±–∫–∞ - –∏—â–µ–º –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ
        print("üîç –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö, –∏—â—É –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ...")

        internet_info = await self.processor.search_internet_fallback(question)

        if internet_info:
            prompt = f"""–¢—ã - —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —ç–∫—Å–ø–µ—Ä—Ç. –û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å, –∏—Å–ø–æ–ª—å–∑—É—è –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞.

–ò–ù–§–û–†–ú–ê–¶–ò–Ø –ò–ó –ò–ù–¢–ï–†–ù–ï–¢–ê:
{internet_info}

–í–û–ü–†–û–°: {question}

–ò–ù–°–¢–†–£–ö–¶–ò–ò:
1. –û—Ç–≤–µ—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
2. –ë—É–¥—å —Ç–æ—á–Ω—ã–º –∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º
3. –£–ø–æ–º—è–Ω–∏, —á—Ç–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –≤–Ω–µ—à–Ω–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤

–û–¢–í–ï–¢:"""

            try:
                response = await self.ask_ollama_with_timeout(
                    messages=[
                        {"role": "system", "content": "–¢—ã - —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —ç–∫—Å–ø–µ—Ä—Ç."},
                        {"role": "user", "content": prompt}
                    ],
                    timeout=15
                )
                answer = response['message']['content']
                answer += "\n\n‚ö†Ô∏è *–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:* –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≤–∑—è—Ç–∞ –∏–∑ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ"
                return answer, "internet", True

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ Ollama –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏: {e}")
                internet_fallback = f"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞:\n{internet_info}"
                return internet_fallback, "internet_raw", True

        else:
            # –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ
            return "‚ùå –ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –Ω–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –Ω–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö, –Ω–∏ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É—Ç–æ—á–Ω–∏—Ç—å –≤–æ–ø—Ä–æ—Å.", "not_found", False

    async def start(self, update: Update, context: CallbackContext):
        """–ö–æ–º–∞–Ω–¥–∞ /start"""
        doc_count = len(self.processor.documents_cache)
        chunk_count = sum(len(chunks) for chunks in self.processor.chunk_index.values())

        welcome_text = f"""
ü§ñ *–£–º–Ω—ã–π PDF Assistant*

üìö *–î–æ–∫—É–º–µ–Ω—Ç–æ–≤:* {doc_count}
üß© *–ß–∞–Ω–∫–æ–≤:* {chunk_count}
üß† *–ú–æ–¥–µ–ª—å:* {OLLAMA_MODEL}

*–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:*
1. –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
2. Fallback –Ω–∞ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
3. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ PDF
4. –£–∫–∞–∑–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏

*–ö–æ–º–∞–Ω–¥—ã:*
/start - —ç—Ç–æ —Å–æ–æ–±—â–µ–Ω–∏–µ
/docs - —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
/reload - –æ–±–Ω–æ–≤–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã
/status - —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞

*–ü—Ä–∏–º–µ—Ä—ã –≤–æ–ø—Ä–æ—Å–æ–≤:*
‚Ä¢ –ß—Ç–æ —Ç–∞–∫–æ–µ ISO 12944?
‚Ä¢ –ö–∞–∫–∏–µ –ì–û–°–¢—ã –ø–æ –ø–æ–∫—Ä–∞—Å–∫–µ –º–µ—Ç–∞–ª–ª–∞?
‚Ä¢ –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏
‚Ä¢ –û–±—ä—è—Å–Ω–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç –ì–û–°–¢ 9.402-2004
"""

        keyboard = [
            [InlineKeyboardButton("üìö –î–æ–∫—É–º–µ–Ω—Ç—ã", callback_data='list_docs')],
            [InlineKeyboardButton("üîÑ –û–±–Ω–æ–≤–∏—Ç—å", callback_data='reload_docs')],
            [InlineKeyboardButton("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞", callback_data='status')]
        ]
        reply_markup = InlineKeyboardMarkup(keyboard)

        if update.message:
            await update.message.reply_text(welcome_text, parse_mode='Markdown', reply_markup=reply_markup)
        elif update.callback_query:
            await update.callback_query.message.reply_text(welcome_text, parse_mode='Markdown',
                                                           reply_markup=reply_markup)

    async def show_documents(self, update: Update, context: CallbackContext):
        """–ü–æ–∫–∞–∑–∞—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º callback_query –µ—Å–ª–∏ –µ—Å—Ç—å
        query = update.callback_query
        if query:
            await query.answer()
            chat_id = query.message.chat_id
            message_id = query.message.message_id
        else:
            # –≠—Ç–æ –∫–æ–º–∞–Ω–¥–∞ –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è
            chat_id = update.effective_chat.id
            message_id = None

        if not self.processor.documents_cache:
            message_text = "üì≠ –í –ø–∞–ø–∫–µ –Ω–µ—Ç PDF-—Ñ–∞–π–ª–æ–≤.\n" \
                           f"–î–æ–±–∞–≤—å—Ç–µ —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫—É `{PDF_FOLDER}`"

            if query:
                await query.edit_message_text(message_text, parse_mode='Markdown')
            else:
                await context.bot.send_message(chat_id, message_text, parse_mode='Markdown')
            return

        doc_list = "üìö *–ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã:*\n\n"
        for filename, doc_data in self.processor.documents_cache.items():
            metadata = doc_data.get("metadata", {})
            pages = metadata.get("pages", 0)
            chunks = doc_data.get("chunk_count", 0)
            method = metadata.get("extraction_method", "unknown")

            doc_list += f"üìÑ *{filename}*\n"
            doc_list += f"   –°—Ç—Ä–∞–Ω–∏—Ü: {pages} | –ß–∞–Ω–∫–æ–≤: {chunks}\n"
            doc_list += f"   –ú–µ—Ç–æ–¥: {method}\n\n"

        keyboard = InlineKeyboardMarkup([[
            InlineKeyboardButton("üîÑ –û–±–Ω–æ–≤–∏—Ç—å", callback_data='reload_docs')
        ]])

        if query:
            await query.edit_message_text(doc_list, parse_mode='Markdown', reply_markup=keyboard)
        else:
            await context.bot.send_message(chat_id, doc_list, parse_mode='Markdown', reply_markup=keyboard)

    async def reload_documents(self, update: Update, context: CallbackContext):
        """–û–±–Ω–æ–≤–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã"""
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º callback_query –µ—Å–ª–∏ –µ—Å—Ç—å
        query = update.callback_query
        if query:
            await query.answer("–û–±–Ω–æ–≤–ª—è—é –¥–æ–∫—É–º–µ–Ω—Ç—ã...")
            chat_id = query.message.chat_id
            message_id = query.message.message_id
            edit_message = True
        else:
            chat_id = update.effective_chat.id
            message_id = None
            edit_message = False

        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –æ –Ω–∞—á–∞–ª–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
        if edit_message:
            message = await query.edit_message_text("üîÑ –û–±–Ω–æ–≤–ª—è—é –¥–æ–∫—É–º–µ–Ω—Ç—ã... –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç.")
        else:
            message = await context.bot.send_message(chat_id,
                                                     "üîÑ –û–±–Ω–æ–≤–ª—è—é –¥–æ–∫—É–º–µ–Ω—Ç—ã... –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç.")

        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
        try:
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(
                executor,
                self.processor.update_documents
            )

            doc_count = len(self.processor.documents_cache)
            message_text = f"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç—ã –æ–±–Ω–æ–≤–ª–µ–Ω—ã!\n–ó–∞–≥—Ä—É–∂–µ–Ω–æ: {doc_count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"

            keyboard = InlineKeyboardMarkup([[
                InlineKeyboardButton("üìö –°–ø–∏—Å–æ–∫", callback_data='list_docs')
            ]])

            if edit_message:
                await query.edit_message_text(message_text, reply_markup=keyboard)
            else:
                await message.edit_text(message_text, reply_markup=keyboard)

        except Exception as e:
            error_msg = f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {str(e)[:100]}"
            if edit_message:
                await query.edit_message_text(error_msg)
            else:
                await message.edit_text(error_msg)

    async def handle_message(self, update: Update, context: CallbackContext):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–æ–ø—Ä–æ—Å–æ–≤"""
        question = update.message.text.strip()

        if not question:
            await update.message.reply_text("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ –≤–æ–ø—Ä–æ—Å.")
            return

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º "–ø–µ—á–∞—Ç–∞–µ—Ç..."
        await context.bot.send_chat_action(
            chat_id=update.effective_chat.id,
            action="typing"
        )

        try:
            # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç —Å fallback
            answer, source_type, success = await self.ask_question_with_fallback(question)

            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            response = f"‚ùì *–í–æ–ø—Ä–æ—Å:* {question}\n\n"
            response += f"ü§ñ *–û—Ç–≤–µ—Ç:*\n{answer}\n\n"

            if source_type == "documents":
                response += "üìö *–ò—Å—Ç–æ—á–Ω–∏–∫:* –î–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –ø–∞–ø–∫–∏"
            elif source_type == "documents_timeout":
                response += "üìö‚è±Ô∏è *–ò—Å—Ç–æ—á–Ω–∏–∫:* –î–æ–∫—É–º–µ–Ω—Ç—ã (–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å —Ç–∞–π–º–∞—É—Ç–æ–º)"
            elif source_type == "internet":
                response += "üåê *–ò—Å—Ç–æ—á–Ω–∏–∫:* –ò–Ω—Ç–µ—Ä–Ω–µ—Ç (–æ—Ç–∫—Ä—ã—Ç—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏)"
            elif source_type == "internet_raw":
                response += "üåê *–ò—Å—Ç–æ—á–Ω–∏–∫:* –ù–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞"
            else:
                response += "‚ö†Ô∏è *–ò—Å—Ç–æ—á–Ω–∏–∫:* –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"

            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç —á–∞—Å—Ç—è–º–∏ –µ—Å–ª–∏ –æ–Ω —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π
            if len(response) > 4000:
                parts = [response[i:i + 4000] for i in range(0, len(response), 4000)]
                for part in parts:
                    await update.message.reply_text(part, parse_mode='Markdown')
                    await asyncio.sleep(0.5)
            else:
                await update.message.reply_text(response, parse_mode='Markdown')

        except asyncio.TimeoutError:
            logger.error(f"–¢–∞–π–º–∞—É—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–æ–ø—Ä–æ—Å–∞: {question}")
            await update.message.reply_text(
                "‚è±Ô∏è *–¢–∞–π–º–∞—É—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏*\n"
                "–ó–∞–ø—Ä–æ—Å –∑–∞–Ω—è–ª —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ:\n"
                "1. –ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å\n"
                "2. –ó–∞–¥–∞—Ç—å –±–æ–ª–µ–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –≤–æ–ø—Ä–æ—Å\n"
                "3. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ Ollama",
                parse_mode='Markdown'
            )
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–æ–ø—Ä–æ—Å–∞: {e}\n{traceback.format_exc()}")
            await update.message.reply_text(f"‚ùå –û—à–∏–±–∫–∞: {str(e)[:200]}")

    async def show_status(self, update: Update, context: CallbackContext):
        """–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç—É—Å"""
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º callback_query –µ—Å–ª–∏ –µ—Å—Ç—å
        query = update.callback_query
        if query:
            await query.answer()
            chat_id = query.message.chat_id
            message_id = query.message.message_id
            edit_message = True
        else:
            chat_id = update.effective_chat.id
            message_id = None
            edit_message = False

        doc_count = len(self.processor.documents_cache)
        chunk_count = sum(len(chunks) for chunks in self.processor.chunk_index.values())
        total_size = sum(doc.get("file_size", 0) for doc in self.processor.documents_cache.values())

        status_text = f"""
üìä *–°—Ç–∞—Ç—É—Å —Å–∏—Å—Ç–µ–º—ã:*

ü§ñ *–ú–æ–¥–µ–ª—å:* {OLLAMA_MODEL}
üìö *–î–æ–∫—É–º–µ–Ω—Ç–æ–≤:* {doc_count}
üß© *–ß–∞–Ω–∫–æ–≤:* {chunk_count}
üíæ *–û–±—â–∏–π —Ä–∞–∑–º–µ—Ä:* {total_size / 1024 / 1024:.1f} MB
‚è±Ô∏è *–¢–∞–π–º–∞—É—Ç Ollama:* {OLLAMA_TIMEOUT} —Å–µ–∫

üìÅ *–ü–∞–ø–∫–∞ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏:* `{PDF_FOLDER}`
"""

        if edit_message:
            await query.edit_message_text(status_text, parse_mode='Markdown')
        else:
            await context.bot.send_message(chat_id, status_text, parse_mode='Markdown')

    async def button_callback(self, update: Update, context: CallbackContext):
        """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–Ω–æ–ø–æ–∫"""
        query = update.callback_query
        await query.answer()

        if query.data == 'list_docs':
            await self.show_documents(update, context)
        elif query.data == 'reload_docs':
            await self.reload_documents(update, context)
        elif query.data == 'status':
            await self.show_status(update, context)

    async def error_handler(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –æ—à–∏–±–æ–∫"""
        logger.error(f"–û—à–∏–±–∫–∞: {context.error}")

        if update and update.effective_chat:
            try:
                await context.bot.send_message(
                    chat_id=update.effective_chat.id,
                    text=f"‚ö†Ô∏è –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {str(context.error)[:200]}"
                )
            except:
                pass

    def run(self):
        """–ó–∞–ø—É—Å–∫ –±–æ—Ç–∞"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º Ollama
        print("üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Ollama...")
        if not self.check_ollama():
            print("‚ùå Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ó–∞–ø—É—Å—Ç–∏—Ç–µ: ollama serve")
            print(f"‚ÑπÔ∏è –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: ollama pull {OLLAMA_MODEL}")
            return

        print("‚úÖ Ollama –¥–æ—Å—Ç—É–ø–µ–Ω!")
        print(f"üìÅ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(self.processor.documents_cache)}")

        # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º–∏ —Ç–∞–π–º–∞—É—Ç–∞–º–∏
        application = Application.builder() \
            .token(self.token) \
            .read_timeout(TELEGRAM_TIMEOUT) \
            .write_timeout(TELEGRAM_TIMEOUT) \
            .connect_timeout(TELEGRAM_TIMEOUT) \
            .pool_timeout(TELEGRAM_TIMEOUT) \
            .build()

        self.application = application

        # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏
        application.add_handler(CommandHandler("start", self.start))
        application.add_handler(CommandHandler("docs", self.show_documents))
        application.add_handler(CommandHandler("reload", self.reload_documents))
        application.add_handler(CommandHandler("status", self.show_status))
        application.add_handler(CallbackQueryHandler(self.button_callback))
        application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, self.handle_message))

        # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –æ—à–∏–±–æ–∫
        application.add_error_handler(self.error_handler)

        # –ó–∞–ø—É—Å–∫–∞–µ–º
        logger.info("ü§ñ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω...")
        print("\n" + "=" * 60)
        print("üöÄ –£–º–Ω—ã–π PDF Assistant –∑–∞–ø—É—â–µ–Ω!")
        print(f"üìÅ –ü–∞–ø–∫–∞: {PDF_FOLDER}")
        print(f"üß† –ú–æ–¥–µ–ª—å: {OLLAMA_MODEL}")
        print(f"üìö –î–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(self.processor.documents_cache)}")
        print(f"‚è±Ô∏è –¢–∞–π–º–∞—É—Ç Ollama: {OLLAMA_TIMEOUT} —Å–µ–∫")
        print("üåê –ò–Ω—Ç–µ—Ä–Ω–µ—Ç: –î–û–°–¢–£–ü–ï–ù")
        print("=" * 60)
        print("\n–ù–∞–∂–º–∏—Ç–µ Ctrl+C –¥–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏.\n")

        # –ó–∞–ø—É—Å–∫–∞–µ–º —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∏—Å–∫–ª—é—á–µ–Ω–∏–π
        try:
            application.run_polling(
                allowed_updates=Update.ALL_TYPES,
                drop_pending_updates=True
            )
        except KeyboardInterrupt:
            print("\n\nüõë –ë–æ—Ç –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        except Exception as e:
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –±–æ—Ç–∞: {e}")
            print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–∫–µ–Ω
    token = os.getenv("TELEGRAM_TOKEN")
    if not token:
        print("ü§ñ –í–≤–µ–¥–∏—Ç–µ —Ç–æ–∫–µ–Ω Telegram –±–æ—Ç–∞ –æ—Ç @BotFather:")
        token = input("Token: ").strip()
        if not token:
            print("‚ùå –¢–æ–∫–µ–Ω –Ω–µ —É–∫–∞–∑–∞–Ω!")
            return
        os.environ["TELEGRAM_TOKEN"] = token

    # –ó–∞–ø—É—Å–∫–∞–µ–º –±–æ—Ç–∞
    bot = SmartPDFAssistant(token)
    bot.run()


if __name__ == "__main__":
    main()